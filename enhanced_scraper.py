#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆçˆ¬è™«è„šæœ¬ - ä¸“é—¨è§£å†³gpt-oss 120Bç¼ºå¤±b200_trtæ•°æ®çš„é—®é¢˜
"""

import json
import time
import os
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains

def setup_driver():
    """è®¾ç½®Chromeé©±åŠ¨"""
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--disable-web-security")
    chrome_options.add_argument("--allow-running-insecure-content")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-plugins")
    chrome_options.add_argument("--disable-images")
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36")

    driver = webdriver.Chrome(options=chrome_options)
    return driver

def setup_comprehensive_network_monitoring(driver):
    """è®¾ç½®å…¨é¢çš„ç½‘ç»œç›‘æ§"""
    script = """
    window.capturedJsonData = [];
    window.requestCounter = 0;
    window.capturedUrls = new Set();
    window.allNetworkRequests = [];

    // å…¨é¢çš„å…³é”®è¯åŒ¹é…
    const keywords = [
        'api', 'data', 'benchmark', 'performance', 'model', 'hardware',
        'inference', 'interactivity', 'e2e', 'latency', 'throughput',
        'gpt', 'llama', 'deepseek', 'b200', 'trt', 'tensorrt', 'gpu'
    ];

    function shouldMonitorUrl(url) {
        const urlStr = url.toLowerCase();
        return keywords.some(keyword => urlStr.includes(keyword)) ||
               urlStr.includes('.json') ||
               urlStr.includes('gpt-oss') ||
               urlStr.includes('b200') ||
               urlStr.includes('tensorrt') ||
               urlStr.includes('inference-performance');
    }

    // æ‹¦æˆªæ‰€æœ‰ç½‘ç»œè¯·æ±‚
    const originalFetch = window.fetch;
    window.fetch = function(...args) {
        const url = args[0];
        const urlStr = typeof url === 'string' ? url : (url.url || '');

        // è®°å½•æ‰€æœ‰è¯·æ±‚
        window.allNetworkRequests.push({
            url: urlStr,
            method: args[1]?.method || 'GET',
            timestamp: Date.now()
        });

        if (shouldMonitorUrl(urlStr)) {
            console.log('ğŸ” Monitoring request:', urlStr);
        }

        return originalFetch.apply(this, args).then(response => {
            if (shouldMonitorUrl(urlStr)) {
                const clonedResponse = response.clone();

                clonedResponse.json().then(data => {
                    window.requestCounter++;
                    const requestId = window.requestCounter;

                    if (!window.capturedUrls.has(urlStr)) {
                        window.capturedUrls.add(urlStr);

                        console.log('âœ… Captured JSON response #' + requestId, 'from:', urlStr);

                        window.capturedJsonData.push({
                            requestId: requestId,
                            url: urlStr,
                            method: args[1]?.method || 'GET',
                            data: data,
                            timestamp: Date.now(),
                            dataSize: JSON.stringify(data).length
                        });
                    }
                }).catch(e => {
                    // å³ä½¿ä¸æ˜¯JSONä¹Ÿè®°å½•
                    console.log('ğŸ“„ Non-JSON response from:', urlStr);
                    window.capturedJsonData.push({
                        requestId: window.requestCounter + 1,
                        url: urlStr,
                        method: args[1]?.method || 'GET',
                        data: null,
                        timestamp: Date.now(),
                        dataSize: 0,
                        nonJson: true
                    });
                });
            }

            return response;
        }).catch(e => {
            console.log('âŒ Fetch error:', e);
            return Promise.reject(e);
        });
    };

    // ä¹Ÿæ‹¦æˆªXMLHttpRequest
    const originalXHROpen = XMLHttpRequest.prototype.open;
    XMLHttpRequest.prototype.open = function(method, url, ...args) {
        this._url = url;
        this._method = method;

        window.allNetworkRequests.push({
            url: url,
            method: method,
            timestamp: Date.now(),
            type: 'XHR'
        });

        return originalXHROpen.call(this, method, url, ...args);
    };

    console.log('ğŸš€ Comprehensive network monitoring activated');
    """

    driver.execute_script(script)

def wait_for_comprehensive_data(driver, timeout=120):
    """ç­‰å¾…æ›´é•¿æ—¶é—´ä»¥æ•è·æ‰€æœ‰æ•°æ®"""
    start_time = time.time()
    last_capture_count = 0
    stable_time = start_time

    print("ğŸ” å¼€å§‹å…¨é¢ç½‘ç»œç›‘æ§...")

    while time.time() - start_time < timeout:
        try:
            current_data = driver.execute_script("return window.capturedJsonData || [];")
            current_count = len(current_data)
            all_requests = driver.execute_script("return window.allNetworkRequests || [];")

            if current_count > last_capture_count:
                last_capture_count = current_count
                stable_time = time.time()
                print(f"ğŸ“Š å·²æ•è· {current_count} ä¸ªJSONå“åº”, {len(all_requests)} ä¸ªæ€»è¯·æ±‚")

            # å¦‚æœ15ç§’å†…æ²¡æœ‰æ–°æ•°æ®ï¼Œè®¤ä¸ºæ•°æ®æ”¶é›†å®Œæˆ
            elif time.time() - stable_time > 15 and current_count > 0:
                print(f"âœ… æ•°æ®æ”¶é›†ç¨³å®šï¼Œå…±æ•è· {current_count} ä¸ªJSONå“åº”")
                break

            time.sleep(2)

        except Exception as e:
            print(f"âš ï¸ ç›‘æ§é”™è¯¯: {e}")
            time.sleep(2)

    if last_capture_count == 0:
        print("âš ï¸ è¶…æ—¶ï¼šæœªæ•è·åˆ°ä»»ä½•JSONæ•°æ®")
        return {"dataCount": 0, "attempts": 0, "timeout": True}

    print(f"ğŸ¯ æˆåŠŸæ•è· {last_capture_count} ä¸ªJSONå“åº”")
    return {"dataCount": last_capture_count, "attempts": 0, "timeout": False}

def comprehensive_click(driver, element, max_retries=5):
    """å…¨é¢çš„ç‚¹å‡»æ–¹æ³•"""
    for attempt in range(max_retries):
        try:
            WebDriverWait(driver, 10).until(EC.visibility_of(element))
            WebDriverWait(driver, 10).until(EC.element_to_be_clickable(element))

            if attempt == 0:
                element.click()
            elif attempt == 1:
                driver.execute_script("arguments[0].click();", element)
            elif attempt == 2:
                actions = ActionChains(driver)
                actions.move_to_element(element).click().perform()
            elif attempt == 3:
                driver.execute_script("""
                    var element = arguments[0];
                    var event = new MouseEvent('click', {
                        'view': window,
                        'bubbles': true,
                        'cancelable': true
                    });
                    element.dispatchEvent(event);
                """, element)
            else:
                driver.execute_script("arguments[0].scrollIntoView(true);", element)
                time.sleep(1)
                element.click()

            print(f"âœ… ç‚¹å‡»æˆåŠŸ (å°è¯• {attempt + 1})")
            return True

        except Exception as e:
            print(f"âš ï¸ ç‚¹å‡»å°è¯• {attempt + 1} å¤±è´¥: {e}")
            if attempt < max_retries - 1:
                time.sleep(1)

    return False

def find_dropdown_comprehensive(driver, search_text):
    """å…¨é¢çš„ä¸‹æ‹‰èœå•æŸ¥æ‰¾"""
    print(f"ğŸ” æŸ¥æ‰¾åŒ…å« '{search_text}' çš„æ§ä»¶...")

    # å°è¯•å¤šç§é€‰æ‹©å™¨å’Œæ–‡æœ¬åŒ¹é…
    strategies = [
        # CSSé€‰æ‹©å™¨
        ('button', lambda el: search_text.lower() in el.text.lower()),
        ('[role="button"]', lambda el: search_text.lower() in el.text.lower()),
        ('select', lambda el: search_text.lower() in el.text.lower()),
        ('option', lambda el: search_text.lower() in el.text.lower()),
        ('.dropdown', lambda el: search_text.lower() in el.text.lower()),
        ('.select', lambda el: search_text.lower() in el.text.lower()),

        # å±æ€§åŒ¹é…
        ('*[data-testid*="select"]', lambda el: True),
        ('*[data-testid*="dropdown"]', lambda el: True),
        ('*[data-test*="select"]', lambda el: True),

        # ç±»ååŒ¹é…
        ('.model-select', lambda el: True),
        ('.hardware-select', lambda el: True),
        ('.sequence-select', lambda el: True),
    ]

    for selector, condition in strategies:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            for element in elements:
                try:
                    text = element.text.strip() or element.get_attribute('value') or ''
                    aria_label = element.get_attribute('aria-label') or ''

                    full_text = (text + ' ' + aria_label).lower()

                    if search_text.lower() in full_text and condition(element):
                        print(f"âœ… æ‰¾åˆ°æ§ä»¶: '{text}' (é€‰æ‹©å™¨: {selector})")
                        return comprehensive_click(driver, element)
                except:
                    continue
        except:
            continue

    # XPathæŸ¥æ‰¾
    try:
        xpath_conditions = [
            f"//*[contains(text(), '{search_text}')]",
            f"//*[contains(@value, '{search_text}')]",
            f"//*[contains(@aria-label, '{search_text}')]",
            f"//*[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{search_text.lower()}')]"
        ]

        for xpath in xpath_conditions:
            elements = driver.find_elements(By.XPATH, xpath)
            for element in elements:
                if comprehensive_click(driver, element):
                    return True
    except:
        pass

    print(f"âŒ æœªæ‰¾åˆ°åŒ…å« '{search_text}' çš„æ§ä»¶")
    return False

def select_option_comprehensive(driver, option_text):
    """å…¨é¢çš„é€‰é¡¹é€‰æ‹©"""
    print(f"ğŸ¯ å°è¯•é€‰æ‹©é€‰é¡¹: '{option_text}'")

    time.sleep(2)

    # ç­‰å¾…å¯èƒ½çš„é€‰é¡¹åŠ è½½
    for _ in range(5):
        try:
            WebDriverWait(driver, 2).until(
                lambda d: len(d.find_elements(By.CSS_SELECTOR, '[role="option"], li, option')) > 0
            )
            break
        except:
            time.sleep(1)

    # å°è¯•å¤šç§é€‰é¡¹é€‰æ‹©ç­–ç•¥
    selectors = [
        '[role="option"]',
        'li',
        'option',
        '.option',
        '[data-option]',
        '.dropdown-item',
        '[role="listbox"] > *',
        'ul > li',
        'div[role="menuitem"]',
        '.select-option'
    ]

    for selector in selectors:
        try:
            options = driver.find_elements(By.CSS_SELECTOR, selector)
            print(f"ğŸ” æ£€æŸ¥é€‰æ‹©å™¨ '{selector}': æ‰¾åˆ° {len(options)} ä¸ªé€‰é¡¹")

            for option in options:
                try:
                    text = option.text.strip() or ''
                    value = option.get_attribute('value') or ''
                    aria_label = option.get_attribute('aria-label') or ''

                    # å…¨é¢çš„æ–‡æœ¬åŒ¹é…
                    full_text = (text + ' ' + value + ' ' + aria_label).lower()
                    search_text = option_text.lower()

                    if (search_text == text.lower() or
                        search_text in text.lower() or
                        search_text == value.lower() or
                        search_text in value.lower() or
                        search_text in aria_label.lower() or
                        text.lower() in search_text):  # æ¨¡ç³ŠåŒ¹é…

                        print(f"âœ… åŒ¹é…åˆ°é€‰é¡¹: '{text}'")
                        return comprehensive_click(driver, option)

                except:
                    continue
        except:
            continue

    # æœ€åå°è¯•JavaScriptæœç´¢å’Œé€‰æ‹©
    script = f"""
    const searchText = '{option_text.toLowerCase()}';
    const allElements = document.querySelectorAll('*');
    const clickableElements = [];

    for (let element of allElements) {{
        const text = element.textContent.toLowerCase().trim();
        const value = (element.value || '').toLowerCase();
        const ariaLabel = (element.getAttribute('aria-label') || '').toLowerCase();
        const role = element.getAttribute('role') || '';
        const tagName = element.tagName.toLowerCase();

        // æ£€æŸ¥æ˜¯å¦ä¸ºå¯ç‚¹å‡»å…ƒç´ 
        const isClickable = (
            role === 'option' ||
            role === 'menuitem' ||
            tagName === 'option' ||
            tagName === 'li' ||
            element.onclick ||
            element.style.cursor === 'pointer'
        );

        // æ£€æŸ¥æ–‡æœ¬åŒ¹é…
        const isMatch = (
            text === searchText ||
            text.includes(searchText) ||
            searchText.includes(text) ||
            value === searchText ||
            value.includes(searchText) ||
            ariaLabel.includes(searchText)
        );

        if (isClickable && isMatch) {{
            clickableElements.push(element);
        }}
    }}

    // å°è¯•ç‚¹å‡»ç¬¬ä¸€ä¸ªåŒ¹é…çš„å…ƒç´ 
    if (clickableElements.length > 0) {{
        clickableElements[0].click();
        return 'success: clicked ' + clickableElements[0].textContent.trim();
    }}

    return 'not_found';
    """

    try:
        result = driver.execute_script(script)
        if result.startswith('success'):
            print(f"âœ… JavaScriptæˆåŠŸé€‰æ‹©: {result}")
            return True
    except Exception as e:
        print(f"âš ï¸ JavaScripté€‰æ‹©å¤±è´¥: {e}")

    print(f"âŒ æœªæ‰¾åˆ°é€‰é¡¹: '{option_text}'")
    return False

def scrape_with_enhanced_method(driver, model, sequence, combo_index):
    """å¢å¼ºçš„çˆ¬å–æ–¹æ³•"""
    print(f"\nğŸš€ å¼€å§‹å¢å¼ºçˆ¬å–ç»„åˆ {combo_index}: {model} + {sequence}")

    result = {
        'combination_index': combo_index,
        'model': model,
        'sequence': sequence,
        'json_responses': [],
        'scraping_issues': [],
        'all_requests': []
    }

    try:
        print("ğŸŒ è®¿é—®ç½‘ç«™...")
        driver.get("https://inferencemax.semianalysis.com/")

        # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
        WebDriverWait(driver, 30).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )

        # é¢å¤–ç­‰å¾…Reactåº”ç”¨åŠ è½½
        time.sleep(5)

        # è®¾ç½®ç½‘ç»œç›‘æ§
        setup_comprehensive_network_monitoring(driver)
        time.sleep(2)

        # æ¨¡æ‹Ÿç”¨æˆ·äº¤äº’ - å…ˆç‚¹å‡»é¡µé¢æ¿€æ´»
        try:
            body = driver.find_element(By.TAG_NAME, 'body')
            body.click()
            time.sleep(1)
        except:
            pass

        # å°è¯•å¤šç§æ–¹æ³•é€‰æ‹©æ¨¡å‹
        print(f"ğŸ“ é€‰æ‹©æ¨¡å‹: {model}")
        model_selectors = ["Model", "model", "Model Selection"]
        model_selected = False

        for selector in model_selectors:
            if find_dropdown_comprehensive(driver, selector):
                time.sleep(1)
                if select_option_comprehensive(driver, model):
                    model_selected = True
                    break

        if not model_selected:
            result['scraping_issues'].append("æ— æ³•é€‰æ‹©æ¨¡å‹")
            return result

        time.sleep(3)

        # é€‰æ‹©åºåˆ—é•¿åº¦
        print(f"ğŸ“ é€‰æ‹©åºåˆ—é•¿åº¦: {sequence}")
        sequence_selectors = ["Sequence", "sequence", "Sequence Length", "Length"]
        sequence_selected = False

        for selector in sequence_selectors:
            if find_dropdown_comprehensive(driver, selector):
                time.sleep(1)
                if select_option_comprehensive(driver, sequence):
                    sequence_selected = True
                    break

        if not sequence_selected:
            result['scraping_issues'].append("æ— æ³•é€‰æ‹©åºåˆ—é•¿åº¦")
            return result

        time.sleep(5)

        # ç­‰å¾…æ•°æ®åŠ è½½
        print("â³ ç­‰å¾…æ•°æ®åŠ è½½...")
        network_result = wait_for_comprehensive_data(driver, timeout=150)

        if network_result['timeout']:
            result['scraping_issues'].append("æ•°æ®åŠ è½½è¶…æ—¶")
        else:
            captured_data = driver.execute_script("return window.capturedJsonData || [];")
            all_requests = driver.execute_script("return window.allNetworkRequests || [];")

            result['json_responses'] = captured_data
            result['all_requests'] = all_requests

            print(f"âœ… æˆåŠŸæ•è· {len(captured_data)} ä¸ªJSONå“åº”, {len(all_requests)} ä¸ªæ€»è¯·æ±‚")

        # ç‰¹åˆ«æ£€æŸ¥gpt-oss 120Bçš„b200_trtæ•°æ®
        if 'gpt-oss' in model.lower():
            b200_trt_count = 0
            all_hwkeys = set()

            for response in captured_data:
                data = response.get('data', [])
                if isinstance(data, list):
                    for item in data:
                        hwkey = item.get('hwKey', '')
                        all_hwkeys.add(str(hwkey))
                        if 'b200_trt' in str(hwkey).lower():
                            b200_trt_count += 1

            print(f"ğŸ” {model} æ¨¡å‹åˆ†æ:")
            print(f"  æ‰€æœ‰hwKeyç±»å‹: {sorted(all_hwkeys)}")
            print(f"  b200_trtæ•°æ®æ¡æ•°: {b200_trt_count}")

            if b200_trt_count == 0 and 'b200_trt' not in str(all_hwkeys):
                result['scraping_issues'].append("ç½‘ç«™å¯èƒ½æ²¡æœ‰gpt-oss 120Bçš„b200_trtæ•°æ®")

    except Exception as e:
        error_msg = f"çˆ¬å–ç»„åˆ {combo_index} æ—¶å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        result['scraping_issues'].append(error_msg)

    return result

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆInferenceMAXçˆ¬è™«...")

    # åªæµ‹è¯•gpt-oss 120Bæ¨¡å‹
    models = ["gpt-oss 120B"]
    sequences = ["1K / 1K", "1K / 8K", "8K / 1K"]

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "json_data/raw_json_files"
    os.makedirs(output_dir, exist_ok=True)

    # è®¾ç½®é©±åŠ¨
    driver = setup_driver()
    success_count = 0
    issue_count = 0

    try:
        combo_index = 4  # ä»4å¼€å§‹ï¼Œå¯¹åº”åŸè„šæœ¬çš„gpt-osså¼€å§‹ç¼–å·

        for model in models:
            for sequence in sequences:
                print(f"\n{'='*80}")
                print(f"ğŸ“Š å¤„ç†ç»„åˆ {combo_index}: {model} + {sequence}")
                print(f"{'='*80}")

                result = scrape_with_enhanced_method(driver, model, sequence, combo_index)

                if result['json_responses']:
                    # ä¿å­˜æ•°æ®
                    for i, response in enumerate(result['json_responses']):
                        data = response.get('data', [])
                        if not data:
                            continue

                        try:
                            file_data = {
                                'metadata': {
                                    'combination_index': combo_index,
                                    'model': model,
                                    'sequence': sequence,
                                    'response_index': i + 1,
                                    'timestamp': response.get('timestamp', time.time()),
                                    'request_id': response.get('requestId', i + 1),
                                    'url': response.get('url', ''),
                                    'data_size': response.get('dataSize', 0)
                                },
                                'data': data
                            }

                            model_safe = model.replace(' ', '_').replace('.', '_')
                            sequence_safe = sequence.replace(' ', '_').replace('/', '___')
                            filename = f"{combo_index:02d}_{model_safe}_{sequence_safe}_{i+1:02d}.json"
                            file_path = os.path.join(output_dir, filename)

                            with open(file_path, 'w', encoding='utf-8') as f:
                                json.dump(file_data, f, ensure_ascii=False, indent=2)

                            print(f"ğŸ’¾ ä¿å­˜æ–‡ä»¶: {filename} ({len(data)} æ¡æ•°æ®)")

                        except Exception as e:
                            print(f"âŒ ä¿å­˜å“åº” {i+1} å¤±è´¥: {str(e)}")

                    success_count += 1
                else:
                    issue_count += 1

                # æ˜¾ç¤ºé—®é¢˜å’Œç½‘ç»œè¯·æ±‚
                if result['scraping_issues']:
                    print(f"âš ï¸ å‘ç°é—®é¢˜:")
                    for issue in result['scraping_issues']:
                        print(f"  - {issue}")

                if result['all_requests']:
                    print(f"ğŸŒ æ•è·åˆ° {len(result['all_requests'])} ä¸ªç½‘ç»œè¯·æ±‚")
                    # æ˜¾ç¤ºå‰10ä¸ªè¯·æ±‚URL
                    for i, req in enumerate(result['all_requests'][:10]):
                        print(f"  {i+1}. {req['url']}")

                combo_index += 1
                time.sleep(3)

    finally:
        driver.quit()

    print(f"\n{'='*80}")
    print("ğŸ“ˆ çˆ¬å–ç»Ÿè®¡:")
    print(f"{'='*80}")
    print(f"æˆåŠŸç»„åˆ: {success_count}")
    print(f"é—®é¢˜ç»„åˆ: {issue_count}")

if __name__ == "__main__":
    main()