#!/usr/bin/env python3
"""
InferenceMAX è‡ªåŠ¨åŒ–æ•°æ®ç®¡é“
ç”¨äºå®šæœŸæŠ“å–ã€å¤„ç†å’Œåˆå¹¶ InferenceMAX ç½‘ç«™çš„AIæ¨ç†æ€§èƒ½æ•°æ®
"""

import os
import sys
import yaml
import json
import shutil
import logging
import traceback
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('/root/semi-bench')

try:
    from api_scraper import scrape_api_data
    from clean_json_files import main as clean_main
    from convert_to_separated_csv import main as convert_main
    from join_csv_files import main as join_main
except ImportError as e:
    print(f"Error importing modules: {e}")
    print("Please ensure all required scripts are in the correct location")
    sys.exit(1)

class InferenceMaxPipeline:
    """InferenceMAX æ•°æ®ç®¡é“ä¸»ç±»"""

    def __init__(self, config_file="config/pipeline_config.yaml"):
        """åˆå§‹åŒ–æ•°æ®ç®¡é“"""
        self.start_time = datetime.now()
        self.pipeline_id = self.start_time.strftime("%Y%m%d_%H%M%S")

        self.config = self.load_config(config_file)
        self.setup_logging()
        self.setup_directories()

        self.logger.info(f"Pipeline {self.pipeline_id} initialized")
        self.logger.info(f"Configuration loaded from: {config_file}")

    def load_config(self, config_file):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_path = Path(__file__).parent.parent / config_file

        if not config_path.exists():
            raise FileNotFoundError(f"Configuration file not found: {config_path}")

        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_config = self.config.get('logging', {})
        log_dir = Path(self.config['paths']['base_dir']) / self.config['paths']['log_dir']
        log_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å
        log_file = log_dir / f"pipeline_{self.pipeline_id}.log"

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, log_config.get('level', 'INFO')),
            format=log_config.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s'),
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )

        self.logger = logging.getLogger('InferenceMaxPipeline')

    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        base_dir = Path(self.config['paths']['base_dir'])

        directories = [
            self.config['paths']['raw_data_dir'],
            self.config['paths']['output_dir'],
            self.config['paths']['archive_dir'],
            self.config['paths']['report_dir']
        ]

        for dir_path in directories:
            full_path = base_dir / dir_path
            full_path.mkdir(parents=True, exist_ok=True)

    def log_step(self, step_name, message=""):
        """è®°å½•æ­¥éª¤æ—¥å¿—"""
        self.logger.info(f"{'='*50}")
        self.logger.info(f"STEP: {step_name}")
        if message:
            self.logger.info(f"INFO: {message}")
        self.logger.info(f"{'='*50}")

    def scrape_data(self):
        """æ­¥éª¤1: ä½¿ç”¨APIç›´æ¥é‡‡é›†æ•°æ®"""
        self.log_step("æ•°æ®é‡‡é›†", "å¼€å§‹ä» InferenceMAX API é‡‡é›†æ•°æ®")

        try:
            # ä¸´æ—¶ä¿®æ”¹å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹ç›®å½•
            original_cwd = os.getcwd()
            os.chdir(self.config['paths']['base_dir'])

            # è·å–é…ç½®
            models = self.config['targets']['models']
            sequences = self.config['targets']['sequences']
            output_dir = self.config['paths']['raw_data_dir']

            self.logger.info(f"æ‰§è¡ŒAPIæ•°æ®é‡‡é›†...")
            self.logger.info(f"ç›®æ ‡: {len(models)} æ¨¡å‹ Ã— {len(sequences)} åºåˆ—")
            self.logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

            # æ‰§è¡ŒAPIé‡‡é›†
            scrape_results = scrape_api_data(models, sequences, output_dir)

            # æ¢å¤å·¥ä½œç›®å½•
            os.chdir(original_cwd)

            # éªŒè¯é‡‡é›†ç»“æœ
            raw_data_dir = Path(self.config['paths']['base_dir']) / output_dir
            json_files = list(raw_data_dir.glob("*.json"))
            json_files = [f for f in json_files if not any(x in f.name.lower()
                       for x in ['readme', 'summary', 'cleanup', 'report', 'api_scraping_summary'])]

            self.logger.info(f"APIé‡‡é›†å®Œæˆï¼Œè·å¾— {len(json_files)} ä¸ªJSONæ–‡ä»¶")
            self.logger.info(f"æ€»è®°å½•æ•°: {scrape_results.get('total_records', 0)}")
            self.logger.info(f"b200_trtæ•°æ®: {scrape_results.get('total_b200_trt', 0)} æ¡")

            # æ£€æŸ¥b200_trtæ•°æ®
            if scrape_results.get('total_b200_trt', 0) == 0:
                self.logger.warning("æœªé‡‡é›†åˆ°ä»»ä½•b200_trtæ•°æ®ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥ç½‘ç»œæˆ–APIç«¯ç‚¹")

            if len(json_files) < 6:  # æœŸæœ›çš„æœ€å°‘æ–‡ä»¶æ•°
                raise ValueError(f"é‡‡é›†çš„æ–‡ä»¶æ•°é‡ä¸è¶³: {len(json_files)} < 6")

            return True

        except Exception as e:
            self.logger.error(f"æ•°æ®çˆ¬å–å¤±è´¥: {str(e)}")
            self.logger.error(traceback.format_exc())
            return False

    def clean_data(self):
        """æ­¥éª¤2: æ¸…ç†æ— æ•ˆæ•°æ®"""
        self.log_step("æ•°æ®æ¸…ç†", "ç§»é™¤æ— æ•ˆå’Œå°æ–‡ä»¶")

        try:
            original_cwd = os.getcwd()
            os.chdir(self.config['paths']['base_dir'])

            self.logger.info("æ‰§è¡Œæ•°æ®æ¸…ç†...")
            clean_main()

            os.chdir(original_cwd)

            # éªŒè¯æ¸…ç†ç»“æœ
            raw_data_dir = Path(self.config['paths']['base_dir']) / self.config['paths']['raw_data_dir']
            valid_files = list(raw_data_dir.glob("*.json"))
            valid_files = [f for f in valid_files if not any(x in f.name.lower()
                          for x in ['readme', 'summary', 'cleanup', 'report'])]

            self.logger.info(f"æ¸…ç†å®Œæˆï¼Œä¿ç•™ {len(valid_files)} ä¸ªæœ‰æ•ˆæ–‡ä»¶")

            return True

        except Exception as e:
            self.logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {str(e)}")
            self.logger.error(traceback.format_exc())
            return False

    def convert_to_csv(self):
        """æ­¥éª¤3: è½¬æ¢ä¸ºåˆ†ç¦»çš„CSVæ–‡ä»¶"""
        self.log_step("CSVè½¬æ¢", "å°†JSONæ•°æ®è½¬æ¢ä¸ºåˆ†ç¦»çš„CSVæ–‡ä»¶")

        try:
            original_cwd = os.getcwd()
            os.chdir(self.config['paths']['base_dir'])

            self.logger.info("æ‰§è¡ŒCSVè½¬æ¢...")
            convert_main()

            os.chdir(original_cwd)

            # éªŒè¯è½¬æ¢ç»“æœ
            output_dir = Path(self.config['paths']['base_dir']) / self.config['paths']['output_dir']
            interactivity_csv = output_dir / "inference_max_interactivity.csv"
            e2e_csv = output_dir / "inference_max_e2e.csv"

            if not interactivity_csv.exists() or not e2e_csv.exists():
                raise FileNotFoundError("CSVè½¬æ¢å¤±è´¥ï¼Œç¼ºå°‘è¾“å‡ºæ–‡ä»¶")

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            inter_size = interactivity_csv.stat().st_size
            e2e_size = e2e_csv.stat().st_size

            self.logger.info(f"è½¬æ¢å®Œæˆ: Interactivity({inter_size:,} bytes), E2E({e2e_size:,} bytes)")

            return True

        except Exception as e:
            self.logger.error(f"CSVè½¬æ¢å¤±è´¥: {str(e)}")
            self.logger.error(traceback.format_exc())
            return False

    def join_csv_files(self):
        """æ­¥éª¤4: åˆå¹¶CSVæ–‡ä»¶"""
        self.log_step("CSVåˆå¹¶", "å°†ä¸¤ä¸ªCSVæ–‡ä»¶åˆå¹¶ä¸ºæœ€ç»ˆæ•°æ®é›†")

        try:
            original_cwd = os.getcwd()
            os.chdir(self.config['paths']['base_dir'])

            self.logger.info("æ‰§è¡ŒCSVåˆå¹¶...")
            join_main()

            os.chdir(original_cwd)

            # éªŒè¯åˆå¹¶ç»“æœ
            output_dir = Path(self.config['paths']['base_dir']) / self.config['paths']['output_dir']
            merged_csv = output_dir / "inference_max_merged.csv"

            if not merged_csv.exists():
                raise FileNotFoundError("CSVåˆå¹¶å¤±è´¥ï¼Œç¼ºå°‘è¾“å‡ºæ–‡ä»¶")

            # æ£€æŸ¥æ–‡ä»¶å¤§å°å’Œè®°å½•æ•°
            file_size = merged_csv.stat().st_size
            self.logger.info(f"åˆå¹¶å®Œæˆ: æœ€ç»ˆæ–‡ä»¶å¤§å° {file_size:,} bytes")

            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            with open(merged_csv, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                record_count = len(lines) - 1  # å‡å»è¡¨å¤´

            expected_min = self.config['monitoring'].get('expected_min_records', 1000)
            if record_count < expected_min:
                self.logger.warning(f"è®°å½•æ•°å¯èƒ½ä¸è¶³: {record_count} < {expected_min}")
            else:
                self.logger.info(f"æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡: {record_count} æ¡è®°å½•")

            return True

        except Exception as e:
            self.logger.error(f"CSVåˆå¹¶å¤±è´¥: {str(e)}")
            self.logger.error(traceback.format_exc())
            return False

    def archive_version(self):
        """ç‰ˆæœ¬æ§åˆ¶å’Œå½’æ¡£"""
        if not self.config['versioning'].get('enabled', False):
            self.logger.info("ç‰ˆæœ¬æ§åˆ¶å·²ç¦ç”¨ï¼Œè·³è¿‡å½’æ¡£")
            return True

        self.log_step("ç‰ˆæœ¬å½’æ¡£", "ä¿å­˜å½“å‰ç‰ˆæœ¬åˆ°å†å²æ¡£æ¡ˆ")

        try:
            base_dir = Path(self.config['paths']['base_dir'])
            archive_dir = base_dir / self.config['paths']['archive_dir']
            output_dir = base_dir / self.config['paths']['output_dir']

            # åˆ›å»ºç‰ˆæœ¬ç›®å½•
            version_dir = archive_dir / f"version_{self.pipeline_id}"
            version_dir.mkdir(exist_ok=True)

            # è¦å½’æ¡£çš„æ–‡ä»¶åˆ—è¡¨
            files_to_archive = [
                "inference_max_interactivity.csv",
                "inference_max_e2e.csv",
                "inference_max_merged.csv",
                "SEPARATED_CSV_CONVERSION_REPORT.md",
                "CSV_MERGE_REPORT.md"
            ]

            archived_files = []
            for file_name in files_to_archive:
                source_file = output_dir / file_name
                if source_file.exists():
                    target_file = version_dir / file_name
                    shutil.copy2(source_file, target_file)
                    archived_files.append(file_name)

                    # å¦‚æœå¯ç”¨å‹ç¼©ï¼Œå‹ç¼©æ–‡ä»¶
                    if self.config['versioning'].get('compression', False):
                        shutil.make_archive(str(target_file.with_suffix('.zip')), 'zip', str(target_file.parent), target_file.name)
                        target_file.unlink()  # åˆ é™¤åŸæ–‡ä»¶

                    self.logger.info(f"å·²å½’æ¡£: {file_name}")

            # åˆ›å»ºç‰ˆæœ¬å…ƒæ•°æ®
            version_metadata = {
                "pipeline_id": self.pipeline_id,
                "timestamp": self.start_time.isoformat(),
                "archived_files": archived_files,
                "config": self.config
            }

            metadata_file = version_dir / "metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(version_metadata, f, indent=2, ensure_ascii=False)

            self.logger.info(f"ç‰ˆæœ¬å½’æ¡£å®Œæˆ: {version_dir}")

            # æ¸…ç†æ—§ç‰ˆæœ¬
            self.cleanup_old_versions()

            return True

        except Exception as e:
            self.logger.error(f"ç‰ˆæœ¬å½’æ¡£å¤±è´¥: {str(e)}")
            self.logger.error(traceback.format_exc())
            return False

    def cleanup_old_versions(self):
        """æ¸…ç†æ—§ç‰ˆæœ¬"""
        max_versions = self.config['versioning'].get('max_versions', 30)
        archive_dir = Path(self.config['paths']['base_dir']) / self.config['paths']['archive_dir']

        version_dirs = [d for d in archive_dir.iterdir() if d.is_dir() and d.name.startswith('version_')]
        version_dirs.sort(reverse=True)  # æŒ‰æ—¶é—´å€’åº

        if len(version_dirs) > max_versions:
            for old_version in version_dirs[max_versions:]:
                try:
                    shutil.rmtree(old_version)
                    self.logger.info(f"å·²åˆ é™¤æ—§ç‰ˆæœ¬: {old_version.name}")
                except Exception as e:
                    self.logger.warning(f"åˆ é™¤æ—§ç‰ˆæœ¬å¤±è´¥ {old_version.name}: {e}")

    def create_final_report(self, success):
        """åˆ›å»ºæœ€ç»ˆæŠ¥å‘Š"""
        self.log_step("ç”ŸæˆæŠ¥å‘Š", f"åˆ›å»ºç®¡é“æ‰§è¡ŒæŠ¥å‘Š (æˆåŠŸ: {success})")

        try:
            report_dir = Path(self.config['paths']['base_dir']) / self.config['paths']['report_dir']
            report_file = report_dir / f"pipeline_report_{self.pipeline_id}.md"

            end_time = datetime.now()
            duration = end_time - self.start_time

            report_content = f"""# InferenceMAX æ•°æ®ç®¡é“æ‰§è¡ŒæŠ¥å‘Š

## æ‰§è¡Œä¿¡æ¯
- **ç®¡é“ID**: {self.pipeline_id}
- **å¼€å§‹æ—¶é—´**: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}
- **ç»“æŸæ—¶é—´**: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
- **æ‰§è¡Œæ—¶é•¿**: {str(duration).split('.')[0]}
- **æ‰§è¡ŒçŠ¶æ€**: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}

## é…ç½®ä¿¡æ¯
- **ç›®æ ‡æ¨¡å‹**: {', '.join(self.config['targets']['models'])}
- **ç›®æ ‡åºåˆ—**: {', '.join(self.config['targets']['sequences'])}
- **ç‰ˆæœ¬æ§åˆ¶**: {'å¯ç”¨' if self.config['versioning']['enabled'] else 'ç¦ç”¨'}

## è¾“å‡ºæ–‡ä»¶
"""

            if success:
                output_dir = Path(self.config['paths']['base_dir']) / self.config['paths']['output_dir']

                final_files = []
                for file_name in ["inference_max_interactivity.csv", "inference_max_e2e.csv", "inference_max_merged.csv"]:
                    file_path = output_dir / file_name
                    if file_path.exists():
                        file_size = file_path.stat().st_size
                        final_files.append(f"- **{file_name}**: {file_size:,} bytes")

                if final_files:
                    report_content += "### ç”Ÿæˆçš„æ–‡ä»¶\n"
                    report_content += "\n".join(final_files) + "\n\n"

                if self.config['versioning']['enabled']:
                    report_content += f"### ç‰ˆæœ¬å½’æ¡£\n"
                    report_content += f"- ç‰ˆæœ¬ç›®å½•: `version_{self.pipeline_id}`\n"
                    report_content += f"- å½’æ¡£ä½ç½®: `{self.config['paths']['archive_dir']}`\n\n"

            report_content += f"""
## æ—¥å¿—æ–‡ä»¶
- **è¯¦ç»†æ—¥å¿—**: `inference_max_pipeline/logs/pipeline_{self.pipeline_id}.log`

## ä¸‹æ¬¡æ‰§è¡Œå»ºè®®
- æ£€æŸ¥æ•°æ®è´¨é‡æ˜¯å¦ç¬¦åˆé¢„æœŸ
- éªŒè¯æ–°æ•°æ®ä¸å†å²æ•°æ®çš„è¶‹åŠ¿å˜åŒ–
- å…³æ³¨æ•°æ®æ›´æ–°é¢‘ç‡å’Œæ¨¡å¼

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}*
"""

            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)

            self.logger.info(f"æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            return True

        except Exception as e:
            self.logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")
            return False

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„ç®¡é“æµç¨‹"""
        self.logger.info(f"å¼€å§‹æ‰§è¡Œ InferenceMAX æ•°æ®ç®¡é“ {self.pipeline_id}")

        success = False

        try:
            # æ­¥éª¤1: æ•°æ®çˆ¬å–
            if not self.scrape_data():
                return False

            # æ­¥éª¤2: æ•°æ®æ¸…ç†
            if not self.clean_data():
                return False

            # æ­¥éª¤3: CSVè½¬æ¢
            if not self.convert_to_csv():
                return False

            # æ­¥éª¤4: CSVåˆå¹¶
            if not self.join_csv_files():
                return False

            # æ­¥éª¤5: ç‰ˆæœ¬å½’æ¡£
            if not self.archive_version():
                return False

            success = True
            self.logger.info("ğŸ‰ æ•°æ®ç®¡é“æ‰§è¡ŒæˆåŠŸï¼")

        except Exception as e:
            self.logger.error(f"ç®¡é“æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            self.logger.error(traceback.format_exc())
            success = False

        finally:
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            self.create_final_report(success)

            end_time = datetime.now()
            duration = end_time - self.start_time

            self.logger.info(f"ç®¡é“æ‰§è¡Œå®Œæˆ")
            self.logger.info(f"æ€»è€—æ—¶: {str(duration).split('.')[0]}")
            self.logger.info(f"æ‰§è¡ŒçŠ¶æ€: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")

        return success

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='InferenceMAX æ•°æ®ç®¡é“')
    parser.add_argument('--config', '-c', default='config/pipeline_config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')

    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–ç®¡é“
        pipeline = InferenceMaxPipeline(args.config)

        # æ‰§è¡Œç®¡é“
        success = pipeline.run()

        # è®¾ç½®é€€å‡ºç 
        sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        print("\nç®¡é“æ‰§è¡Œè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()