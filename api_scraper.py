#!/usr/bin/env python3
"""
åŸºäºAPIçš„ç›´æ¥æ•°æ®é‡‡é›†å™¨ - æ›¿æ¢åŸæœ‰çš„ç½‘é¡µçˆ¬å–æ–¹æ³•
"""

import requests
import json
import time
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import logging

class APIDataCollector:
    """APIæ•°æ®é‡‡é›†å™¨"""

    def __init__(self):
        self.base_url = "https://inferencemax.semianalysis.com/data/inference-performance"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

    def normalize_model_name(self, model: str) -> str:
        """æ ‡å‡†åŒ–æ¨¡å‹åç§°ç”¨äºURL"""
        normalized = model.lower().replace(' ', '-')
        # å…ˆå¤„ç†3.3 -> 3_3ï¼Œå†å¤„ç†å…¶ä»–çš„. -> -
        normalized = normalized.replace('3.3', '3_3')
        normalized = normalized.replace('.', '-')
        return normalized

    def normalize_sequence_name(self, sequence: str) -> str:
        """æ ‡å‡†åŒ–åºåˆ—åç§°ç”¨äºURL"""
        return sequence.lower().replace(' / ', '_').replace('/', '_')

    def normalize_llama_sequence_name(self, sequence: str) -> str:
        """ä¸ºLlamaæ¨¡å‹æ ‡å‡†åŒ–åºåˆ—åç§°ï¼ŒåŒ¹é…ç‰¹å®šçš„URLæ ¼å¼"""
        # å¤„ç† "1K / 1K" -> "1k_1k", "1K / 8K" -> "1k_8k", "8K / 1K" -> "8k_1k"
        return sequence.lower().replace(' ', '').replace('/', '_')

    def _generate_url(self, model: str, sequence: str, data_type: str, precision: str = 'fp8') -> str:
        """ç”Ÿæˆè¯·æ±‚URLï¼Œä¸fetch_dataæ–¹æ³•ä¿æŒä¸€è‡´"""
        model_url = self.normalize_model_name(model)

        if 'llama' in model_url.lower():
            sequence_url = self.normalize_llama_sequence_name(sequence)
            return f"{self.base_url}/{model_url}-{precision}-{sequence_url}-{data_type}.json"
        else:
            sequence_url = self.normalize_sequence_name(sequence)
            return f"{self.base_url}/{model_url}-{sequence_url}-{data_type}.json"

    def fetch_data(self, model: str, sequence: str, data_type: str) -> Tuple[Optional[List], bool]:
        """è·å–æŒ‡å®šæ•°æ®"""
        model_url = self.normalize_model_name(model)

        # ä¸ºLlamaæ¨¡å‹ä½¿ç”¨ç‰¹æ®Šçš„åºåˆ—åç§°æ ¼å¼
        if 'llama' in model_url.lower():
            sequence_url = self.normalize_llama_sequence_name(sequence)
            url = f"{self.base_url}/{model_url}-fp8-{sequence_url}-{data_type}.json"
        else:
            sequence_url = self.normalize_sequence_name(sequence)
            url = f"{self.base_url}/{model_url}-{sequence_url}-{data_type}.json"

        try:
            response = self.session.get(url, timeout=30)

            if response.status_code == 200:
                data = response.json()
                return data, True
            else:
                logging.warning(f"HTTP {response.status_code} for {url}")
                return None, False

        except Exception as e:
            logging.error(f"Failed to fetch {url}: {str(e)}")
            return None, False

    def analyze_data(self, data: List[Dict]) -> Dict:
        """åˆ†ææ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if not data:
            return {
                'record_count': 0,
                'hwkeys': set(),
                'b200_trt_count': 0,
                'has_b200_trt': False
            }

        hwkeys = set()
        b200_trt_count = 0

        for item in data:
            hwkey = item.get('hwKey', '')
            hwkeys.add(str(hwkey))
            if 'b200_trt' in str(hwkey).lower():
                b200_trt_count += 1

        return {
            'record_count': len(data),
            'hwkeys': hwkeys,
            'b200_trt_count': b200_trt_count,
            'has_b200_trt': b200_trt_count > 0
        }

    def save_json_file(self, data: List[Dict], model: str, sequence: str, data_type: str,
                      output_dir: str, response_index: int = 1) -> str:
        """ä¿å­˜JSONæ–‡ä»¶"""
        os.makedirs(output_dir, exist_ok=True)

        model_safe = model.replace(' ', '_').replace('.', '_')
        sequence_safe = sequence.replace(' ', '_').replace('/', '___')

        filename = f"{response_index:02d}_{model_safe}_{sequence_safe}_{data_type}.json"
        filepath = os.path.join(output_dir, filename)

        # åˆ†ææ•°æ®
        analysis = self.analyze_data(data)

        file_data = {
            'metadata': {
                'combination_index': response_index,
                'model': model,
                'sequence': sequence,
                'response_index': response_index,
                'timestamp': datetime.now().isoformat(),
                'request_id': response_index,
                'url': self._generate_url(model, sequence, data_type),
                'method': 'GET',
                'content_type': 'application/json',
                'data_size': len(json.dumps(data)),
                'data_type': data_type,
                'record_count': analysis['record_count'],
                'b200_trt_count': analysis['b200_trt_count'],
                'hwkeys': sorted(list(analysis['hwkeys']))
            },
            'data': data
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(file_data, f, indent=2, ensure_ascii=False)

        logging.info(f"Saved: {filename} ({analysis['record_count']} records, {analysis['b200_trt_count']} b200_trt)")
        return filepath

    def collect_all_data(self, models: List[str], sequences: List[str],
                        output_dir: str) -> Dict:
        """é‡‡é›†æ‰€æœ‰æ•°æ®"""
        data_types = ["e2e", "interactivity"]
        results = {
            'successful_collections': [],
            'failed_collections': [],
            'total_files': 0,
            'total_records': 0,
            'total_b200_trt': 0,
            'model_stats': {}
        }

        for model in models:
            model_stats = {
                'files': 0,
                'records': 0,
                'b200_trt': 0,
                'hwkeys': set(),
                'successful_combinations': 0
            }

            combination_index = 1

            for sequence in sequences:
                combination_data = {
                    'model': model,
                    'sequence': sequence,
                    'data_types': {},
                    'timestamp': datetime.now().isoformat()
                }

                combination_success = False

                for data_type in data_types:
                    print(f"\nğŸ“Š Collecting {model} + {sequence} ({data_type})...")

                    data, success = self.fetch_data(model, sequence, data_type)

                    if success and data:
                        try:
                            # ä¿å­˜æ–‡ä»¶
                            filepath = self.save_json_file(data, model, sequence, data_type,
                                                         output_dir, combination_index)

                            # åˆ†ææ•°æ®
                            analysis = self.analyze_data(data)

                            # æ›´æ–°ç»Ÿè®¡
                            model_stats['files'] += 1
                            model_stats['records'] += analysis['record_count']
                            model_stats['b200_trt'] += analysis['b200_trt_count']
                            model_stats['hwkeys'].update(analysis['hwkeys'])

                            results['total_files'] += 1
                            results['total_records'] += analysis['record_count']
                            results['total_b200_trt'] += analysis['b200_trt_count']

                            combination_data['data_types'][data_type] = {
                                'success': True,
                                'record_count': analysis['record_count'],
                                'b200_trt_count': analysis['b200_trt_count'],
                                'hwkeys': sorted(list(analysis['hwkeys'])),
                                'filepath': filepath
                            }

                            combination_success = True
                            print(f"âœ… Success: {analysis['record_count']} records, {analysis['b200_trt_count']} b200_trt")

                        except Exception as e:
                            print(f"âŒ Failed to save file: {str(e)}")
                            combination_data['data_types'][data_type] = {
                                'success': False,
                                'error': str(e)
                            }
                    else:
                        print(f"âŒ Failed to fetch data")
                        combination_data['data_types'][data_type] = {
                            'success': False,
                            'error': 'Failed to fetch data'
                        }

                    time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«

                if combination_success:
                    model_stats['successful_combinations'] += 1
                    results['successful_collections'].append(combination_data)
                else:
                    results['failed_collections'].append(combination_data)

                combination_index += 1

            # è½¬æ¢hwkeysä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
            model_stats['hwkeys'] = sorted(list(model_stats['hwkeys']))
            results['model_stats'][model] = model_stats

        return results


def scrape_api_data(models: List[str], sequences: List[str],
                    output_dir: str = "json_data/raw_json_files") -> Dict:
    """
    ä½¿ç”¨APIæ–¹æ³•é‡‡é›†æ•°æ®çš„å…¥å£å‡½æ•°

    Args:
        models: æ¨¡å‹åˆ—è¡¨
        sequences: åºåˆ—é•¿åº¦åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        é‡‡é›†ç»“æœå­—å…¸
    """
    print("ğŸš€ Starting API-based InferenceMAX data collection...")
    print(f"ğŸ“‹ Target: {len(models)} models Ã— {len(sequences)} sequences = {len(models) * len(sequences)} combinations")
    print(f"ğŸ“ Output directory: {output_dir}")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # åˆ›å»ºé‡‡é›†å™¨
    collector = APIDataCollector()

    # å¼€å§‹é‡‡é›†
    start_time = time.time()
    results = collector.collect_all_data(models, sequences, output_dir)
    elapsed_time = time.time() - start_time

    # æ›´æ–°ç»Ÿè®¡
    results['elapsed_time'] = elapsed_time
    results['timestamp'] = datetime.now().isoformat()

    # æ‰“å°ç»“æœ
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ Collection Statistics:")
    print(f"{'='*80}")
    print(f"Elapsed time: {elapsed_time:.1f} seconds")
    print(f"Total files: {results['total_files']}")
    print(f"Total records: {results['total_records']}")
    print(f"Total b200_trt data: {results['total_b200_trt']}")
    print(f"Successful combinations: {len(results['successful_collections'])}")

    print(f"\nğŸ“Š Model Details:")
    for model, stats in results['model_stats'].items():
        print(f"\nğŸ”¸ {model}:")
        print(f"  Files: {stats['files']}")
        print(f"  Records: {stats['records']}")
        print(f"  b200_trt: {stats['b200_trt']} ({'âœ…' if stats['b200_trt'] > 0 else 'âŒ'})")
        print(f"  Hardware: {stats['hwkeys']}")

    # ä¿å­˜æ€»ç»“æŠ¥å‘Š
    summary_file = os.path.join(output_dir, 'api_scraping_summary.json')
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“‹ Summary saved: {summary_file}")

    return results


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # å®šä¹‰è¦é‡‡é›†çš„æ•°æ®
    models = [
        "Llama 3.3 70B Instruct",
        "gpt-oss 120B",
        "DeepSeek R1 0528"
    ]

    sequences = ["1K / 1K", "1K / 8K", "8K / 1K"]

    # æ‰§è¡Œé‡‡é›†
    results = scrape_api_data(models, sequences)