#!/usr/bin/env python3
import json
import os
import glob
import csv
import re
from datetime import datetime

def normalize_sequence_format(sequence_str):
    """å°†åºåˆ—æ ¼å¼æ ‡å‡†åŒ–ä¸º 1k-1k, 1k-8k ç­‰æ ¼å¼"""
    normalized = sequence_str.replace(' ', '').lower()
    normalized = normalized.replace('/', '-')
    return normalized

def categorize_json_file(filepath):
    """æ ¹æ®URLåˆ¤æ–­JSONæ–‡ä»¶ç±»å‹"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        url = data.get('metadata', {}).get('url', '')

        if 'interactivity.json' in url:
            return 'interactivity'
        elif 'e2e.json' in url:
            return 'e2e'
        else:
            print(f"âš ï¸  Warning: Unknown file type for {filepath}")
            return None

    except Exception as e:
        print(f"Error categorizing {filepath}: {e}")
        return None

def extract_all_fields(json_files):
    """æå–æ‰€æœ‰å¯èƒ½çš„å­—æ®µå"""
    all_fields = set()
    all_nested_fields = {}

    for filepath in json_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            data_points = data.get('data', [])
            for point in data_points:
                if isinstance(point, dict):
                    for key, value in point.items():
                        if isinstance(value, dict):
                            for nested_key in value.keys():
                                all_nested_fields[key] = all_nested_fields.get(key, set())
                                all_nested_fields[key].add(nested_key)
                        else:
                            all_fields.add(key)
        except Exception as e:
            print(f"Error extracting fields from {filepath}: {e}")

    # å°†åµŒå¥—å­—æ®µè½¬æ¢ä¸ºæ‰å¹³å­—æ®µå
    flat_fields = set(all_fields)
    for parent_key, nested_keys in all_nested_fields.items():
        for nested_key in nested_keys:
            flat_fields.add(f"{parent_key}_{nested_key}")

    return sorted(list(flat_fields))

def flatten_data_point(data_point):
    """å°†åµŒå¥—çš„æ•°æ®ç‚¹æ‰å¹³åŒ–"""
    flattened = {}

    for key, value in data_point.items():
        if isinstance(value, dict):
            for nested_key, nested_value in value.items():
                flattened[f"{key}_{nested_key}"] = nested_value
        else:
            flattened[key] = value

    return flattened

def process_json_files_by_type(directory):
    """æŒ‰ç±»å‹å¤„ç†JSONæ–‡ä»¶"""
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶ï¼ˆæ’é™¤æŠ¥å‘Šæ–‡ä»¶ï¼‰
    json_files = glob.glob(os.path.join(directory, '*.json'))
    json_files = [f for f in json_files if not any(x in os.path.basename(f).lower()
                                               for x in ['readme', 'summary', 'cleanup', 'report'])]

    print(f"ğŸ“Š Found {len(json_files)} JSON files to process")

    if not json_files:
        print("âŒ No JSON files found!")
        return None, None

    # æŒ‰ç±»å‹åˆ†ç±»æ–‡ä»¶
    interactivity_files = []
    e2e_files = []

    print("ğŸ” Categorizing files by type...")
    for filepath in json_files:
        file_type = categorize_json_file(filepath)
        if file_type == 'interactivity':
            interactivity_files.append(filepath)
        elif file_type == 'e2e':
            e2e_files.append(filepath)

    print(f"ğŸ“‹ Interactivity files: {len(interactivity_files)}")
    print(f"ğŸ“‹ E2E files: {len(e2e_files)}")

    if len(interactivity_files) != len(e2e_files):
        print(f"âš ï¸  Warning: Different number of files - Interactivity: {len(interactivity_files)}, E2E: {len(e2e_files)}")

    return interactivity_files, e2e_files

def convert_files_to_csv(files, file_type, output_file):
    """å°†æŒ‡å®šç±»å‹çš„æ–‡ä»¶è½¬æ¢ä¸ºCSV"""
    if not files:
        print(f"âŒ No {file_type} files to process")
        return None

    print(f"\nğŸ”„ Processing {file_type} files...")

    # æå–æ‰€æœ‰å¯èƒ½çš„å­—æ®µ
    print("ğŸ” Analyzing data structure...")
    all_fields = extract_all_fields(files)
    print(f"ğŸ“‹ Found {len(all_fields)} unique data fields for {file_type}")

    # å®šä¹‰CSVåˆ—çš„é¡ºåº
    csv_columns = ['model_name', 'sequence_length'] + all_fields

    # å‡†å¤‡CSVæ•°æ®
    csv_data = []
    total_records = 0

    for i, filepath in enumerate(files):
        filename = os.path.basename(filepath)
        print(f"  ğŸ“„ Processing {i+1}/{len(files)}: {filename}")

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                json_data = json.load(f)

            # æå–å…ƒæ•°æ®
            metadata = json_data.get('metadata', {})
            model_name = metadata.get('model', 'Unknown')
            sequence_length = normalize_sequence_format(metadata.get('sequence', 'Unknown'))

            # å¤„ç†æ•°æ®ç‚¹
            data_points = json_data.get('data', [])
            file_records = 0

            for data_point in data_points:
                if isinstance(data_point, dict):
                    # æ‰å¹³åŒ–æ•°æ®ç‚¹
                    flattened_point = flatten_data_point(data_point)

                    # åˆ›å»ºCSVè¡Œ
                    csv_row = {}

                    # æ·»åŠ æ¨¡å‹å’Œåºåˆ—åˆ—
                    csv_row['model_name'] = model_name
                    csv_row['sequence_length'] = sequence_length

                    # æ·»åŠ æ‰€æœ‰æ•°æ®å­—æ®µ
                    for field in all_fields:
                        csv_row[field] = flattened_point.get(field, '')

                    csv_data.append(csv_row)
                    file_records += 1

            total_records += file_records
            print(f"    âœ… Extracted {file_records} data points")

        except Exception as e:
            print(f"    âŒ Error processing {filename}: {e}")

    print(f"ğŸ“Š Total {file_type} records extracted: {total_records}")

    # ä¿å­˜CSVæ–‡ä»¶
    if save_csv_file(csv_columns, csv_data, output_file):
        return {
            'columns': csv_columns,
            'data': csv_data,
            'total_records': total_records,
            'file_count': len(files)
        }
    else:
        return None

def save_csv_file(csv_columns, csv_data, output_file):
    """ä¿å­˜CSVæ–‡ä»¶"""
    try:
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=csv_columns)

            # å†™å…¥è¡¨å¤´
            writer.writeheader()

            # å†™å…¥æ•°æ®è¡Œ
            writer.writerows(csv_data)

        print(f"âœ… CSV file saved: {output_file}")
        return True

    except Exception as e:
        print(f"âŒ Error saving CSV file: {e}")
        return False

def create_comprehensive_summary(interactivity_result, e2e_result):
    """åˆ›å»ºç»¼åˆæŠ¥å‘Š"""
    summary = {
        'generation_timestamp': datetime.now().isoformat(),
        'interactivity': {
            'output_file': 'json_data/inference_max_interactivity.csv',
            'total_records': interactivity_result['total_records'] if interactivity_result else 0,
            'file_count': interactivity_result['file_count'] if interactivity_result else 0,
            'columns': len(interactivity_result['columns']) if interactivity_result else 0
        },
        'e2e': {
            'output_file': 'json_data/inference_max_e2e.csv',
            'total_records': e2e_result['total_records'] if e2e_result else 0,
            'file_count': e2e_result['file_count'] if e2e_result else 0,
            'columns': len(e2e_result['columns']) if e2e_result else 0
        }
    }

    # ç»Ÿè®¡æ¨¡å‹å’Œåºåˆ—ä¿¡æ¯
    all_data = []
    if interactivity_result:
        all_data.extend(interactivity_result['data'])
    if e2e_result:
        all_data.extend(e2e_result['data'])

    if all_data:
        models = set()
        sequences = set()
        hardware = set()
        precisions = set()

        for row in all_data:
            models.add(row.get('model_name', 'Unknown'))
            sequences.add(row.get('sequence_length', 'Unknown'))
            if row.get('hwKey'):
                hardware.add(row['hwKey'])
            if row.get('precision'):
                precisions.add(row['precision'])

        summary['overview'] = {
            'unique_models': list(models),
            'unique_sequences': list(sequences),
            'unique_hardware': list(hardware),
            'unique_precisions': list(precisions),
            'total_combined_records': len(all_data)
        }

    return summary

def create_readable_summary(summary):
    """åˆ›å»ºå¯è¯»çš„æ‘˜è¦æ–‡ä»¶"""
    summary_text = f"""# InferenceMAX åˆ†ç¦»æ•°æ®é›† CSV è½¬æ¢æŠ¥å‘Š

## ğŸ“Š è½¬æ¢ç»Ÿè®¡
- **ç”Ÿæˆæ—¶é—´**: {summary['generation_timestamp']}
- **æ€»è®°å½•æ•°**: {summary.get('overview', {}).get('total_combined_records', 0):,}

## ğŸ“„ Interactivity æ•°æ®é›†
- **è¾“å‡ºæ–‡ä»¶**: {summary['interactivity']['output_file']}
- **è®°å½•æ•°**: {summary['interactivity']['total_records']:,}
- **æ–‡ä»¶æ•°**: {summary['interactivity']['file_count']}
- **åˆ—æ•°**: {summary['interactivity']['columns']}

## ğŸ“„ E2E æ•°æ®é›†
- **è¾“å‡ºæ–‡ä»¶**: {summary['e2e']['output_file']}
- **è®°å½•æ•°**: {summary['e2e']['total_records']:,}
- **æ–‡ä»¶æ•°**: {summary['e2e']['file_count']}
- **åˆ—æ•°**: {summary['e2e']['columns']}

## ğŸ–¥ï¸ æ•°æ®æ¦‚è§ˆ
"""

    if 'overview' in summary:
        overview = summary['overview']
        summary_text += f"""- **æ¨¡å‹æ•°é‡**: {len(overview['unique_models'])}
- **åºåˆ—é…ç½®**: {len(overview['unique_sequences'])}
- **ç¡¬ä»¶å¹³å°**: {len(overview['unique_hardware'])}
- **ç²¾åº¦ç±»å‹**: {len(overview['unique_precisions'])}

### æ¨¡å‹åˆ—è¡¨
"""
        for model in sorted(overview['unique_models']):
            summary_text += f"- {model}\n"

        summary_text += f"""
### åºåˆ—é…ç½®
"""
        for seq in sorted(overview['unique_sequences']):
            summary_text += f"- {seq}\n"

        summary_text += f"""
### ç¡¬ä»¶å¹³å°
"""
        for hw in sorted(overview['unique_hardware']):
            summary_text += f"- {hw}\n"

        summary_text += f"""
### ç²¾åº¦ç±»å‹
"""
        for precision in sorted(overview['unique_precisions']):
            summary_text += f"- {precision}\n"

    summary_text += f"""
## ğŸ“‹ æ•°æ®ç±»å‹è¯´æ˜

### Interactivity æ•°æ®
- **ç”¨é€”**: äº¤äº’å¼æ¨ç†æ€§èƒ½åˆ†æ
- **xå­—æ®µå«ä¹‰**: å»¶è¿Ÿæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
- **yå­—æ®µå«ä¹‰**: ååé‡ï¼ˆtokens/secondï¼‰
- **é€‚ç”¨åœºæ™¯**: å®æ—¶åº”ç”¨ã€èŠå¤©æœºå™¨äººç­‰éœ€è¦ä½å»¶è¿Ÿçš„åœºæ™¯

### E2E (End-to-End) æ•°æ®
- **ç”¨é€”**: ç«¯åˆ°ç«¯æ¨ç†æ€§èƒ½åˆ†æ
- **xå­—æ®µå«ä¹‰**: å…¶ä»–æ€§èƒ½æŒ‡æ ‡ï¼ˆå¯èƒ½æ˜¯æˆæœ¬æˆ–å…¶ä»–åº¦é‡ï¼‰
- **yå­—æ®µå«ä¹‰**: ç›¸åº”çš„æ€§èƒ½å€¼
- **é€‚ç”¨åœºæ™¯**: æ‰¹å¤„ç†ã€ç¦»çº¿åˆ†æç­‰ç»¼åˆè€ƒè™‘çš„åœºæ™¯

## ğŸ“„ CSV åˆ—è¯´æ˜
æ¯ä¸ªCSVæ–‡ä»¶åŒ…å«ä»¥ä¸‹åˆ—ï¼š
- **model_name**: æ¨¡å‹åç§°
- **sequence_length**: åºåˆ—é•¿åº¦é…ç½® (1k-1k, 1k-8k, 8k-1k)
- **conc**: å¹¶å‘æ•°
- **hwKey**: ç¡¬ä»¶å¹³å°æ ‡è¯†
- **precision**: ç²¾åº¦ç±»å‹ (FP8/FP4)
- **tp**: å¼ é‡å¹¶è¡Œæ•°
- **costh_y**: P50å»¶è¿Ÿ (æ¯«ç§’)
- **costn_y**: P90å»¶è¿Ÿ (æ¯«ç§’)
- **costr_y**: P99å»¶è¿Ÿ (æ¯«ç§’)
- **tpPerGpu_y**: æ¯GPUååé‡ (tokens/second)
- **tpPerMw_y**: æ¯å…†ç“¦ååé‡
- **x/y**: åæ ‡å€¼ï¼ˆå«ä¹‰æ ¹æ®æ•°æ®ç±»å‹ä¸åŒï¼‰
- **å…¶ä»–å­—æ®µ**: å„ç§æ€§èƒ½ä¸Šé™æ ‡å¿—

## ğŸ” ä½¿ç”¨å»ºè®®
1. **äº¤äº’å¼åœºæ™¯**: ä½¿ç”¨ interactivity.csv è¿›è¡Œå»¶è¿Ÿæ•æ„Ÿçš„åˆ†æ
2. **æ‰¹å¤„ç†åœºæ™¯**: ä½¿ç”¨ e2e.csv è¿›è¡Œç»¼åˆæ€§èƒ½åˆ†æ
3. **æ€§èƒ½æ¯”è¾ƒ**: å¯ä»¥åˆ†åˆ«å¯¹æ¯”ä¸¤ç§ç±»å‹çš„æ•°æ®
4. **ç¡¬ä»¶é€‰æ‹©**: æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©åˆé€‚çš„æ•°æ®é›†

---

*æ­¤CSVæ–‡ä»¶ç”± InferenceMAX JSONæ•°æ®åˆ†ç¦»è½¬æ¢ç”Ÿæˆ*
"""

    return summary_text

def main():
    input_directory = 'json_data/raw_json_files'
    interactivity_output = 'json_data/inference_max_interactivity.csv'
    e2e_output = 'json_data/inference_max_e2e.csv'
    summary_file = 'json_data/SEPARATED_CSV_CONVERSION_REPORT.md'

    if not os.path.exists(input_directory):
        print(f"âŒ Input directory {input_directory} does not exist")
        return

    print("ğŸš€ Starting separated JSON to CSV conversion...")

    # æŒ‰ç±»å‹åˆ†ç±»æ–‡ä»¶
    interactivity_files, e2e_files = process_json_files_by_type(input_directory)

    if not interactivity_files and not e2e_files:
        print("âŒ No valid files found")
        return

    # è½¬æ¢interactivityæ–‡ä»¶
    interactivity_result = None
    if interactivity_files:
        print(f"\nğŸ“Š Converting Interactivity files...")
        interactivity_result = convert_files_to_csv(interactivity_files, 'interactivity', interactivity_output)

    # è½¬æ¢e2eæ–‡ä»¶
    e2e_result = None
    if e2e_files:
        print(f"\nğŸ“Š Converting E2E files...")
        e2e_result = convert_files_to_csv(e2e_files, 'e2e', e2e_output)

    if not interactivity_result and not e2e_result:
        print("âŒ No data was converted")
        return

    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    summary = create_comprehensive_summary(interactivity_result, e2e_result)

    # åˆ›å»ºå¯è¯»æ‘˜è¦
    readable_summary = create_readable_summary(summary)

    # ä¿å­˜æ‘˜è¦æ–‡ä»¶
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(readable_summary)

    print(f"\nğŸ“‹ Summary report saved: {summary_file}")

    # æ˜¾ç¤ºè½¬æ¢ç»“æœ
    print(f"\nğŸ‰ Separated conversion completed successfully!")

    if interactivity_result:
        file_size = os.path.getsize(interactivity_output)
        print(f"ğŸ“„ Interactivity CSV: {interactivity_output}")
        print(f"ğŸ“Š Size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")
        print(f"ğŸ“ˆ Records: {interactivity_result['total_records']:,}")

    if e2e_result:
        file_size = os.path.getsize(e2e_output)
        print(f"ğŸ“„ E2E CSV: {e2e_output}")
        print(f"ğŸ“Š Size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")
        print(f"ğŸ“ˆ Records: {e2e_result['total_records']:,}")

    total_records = (interactivity_result['total_records'] if interactivity_result else 0) + \
                   (e2e_result['total_records'] if e2e_result else 0)
    print(f"ğŸ“Š Total records across both files: {total_records:,}")

if __name__ == "__main__":
    main()