#!/usr/bin/env python3
import json
import os
import glob
from pathlib import Path

def analyze_json_file(filepath):
    """åˆ†æå•ä¸ªJSONæ–‡ä»¶çš„è´¨é‡"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # è·å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(filepath)

        # æ£€æŸ¥åŸºæœ¬ç»“æ„
        if not isinstance(data, dict):
            return {
                'valid': False,
                'reason': 'Root is not a dictionary',
                'file_size': file_size
            }

        # æ£€æŸ¥metadataå’Œdataå­—æ®µ
        if 'metadata' not in data or 'data' not in data:
            return {
                'valid': False,
                'reason': 'Missing metadata or data field',
                'file_size': file_size
            }

        # æ£€æŸ¥dataå­—æ®µæ˜¯å¦ä¸ºç©º
        data_content = data.get('data', [])
        if not data_content:
            return {
                'valid': False,
                'reason': 'Data field is empty',
                'file_size': file_size
            }

        # æ£€æŸ¥dataå­—æ®µæ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®ç‚¹
        valid_data_points = 0
        for item in data_content:
            if isinstance(item, dict):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®çš„æ€§èƒ½æŒ‡æ ‡
                if any(key in item for key in ['conc', 'tpPerGpu', 'hwKey', 'precision']):
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å€¼æ•°æ®
                    has_numeric_values = False
                    for key, value in item.items():
                        if isinstance(value, (int, float)) and value > 0:
                            has_numeric_values = True
                            break

                    if has_numeric_values:
                        valid_data_points += 1

        if valid_data_points == 0:
            return {
                'valid': False,
                'reason': 'No valid data points with numeric values',
                'file_size': file_size,
                'total_data_points': len(data_content)
            }

        # æ£€æŸ¥æ–‡ä»¶å¤§å°é˜ˆå€¼ï¼ˆå°äº1KBé€šå¸¸æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼‰
        if file_size < 1024:
            return {
                'valid': False,
                'reason': f'File too small ({file_size} bytes)',
                'file_size': file_size,
                'valid_data_points': valid_data_points
            }

        return {
            'valid': True,
            'file_size': file_size,
            'total_data_points': len(data_content),
            'valid_data_points': valid_data_points,
            'metadata': data.get('metadata', {})
        }

    except json.JSONDecodeError as e:
        return {
            'valid': False,
            'reason': f'JSON decode error: {str(e)}',
            'file_size': os.path.getsize(filepath) if os.path.exists(filepath) else 0
        }
    except Exception as e:
        return {
            'valid': False,
            'reason': f'Error reading file: {str(e)}',
            'file_size': os.path.getsize(filepath) if os.path.exists(filepath) else 0
        }

def analyze_all_files(directory):
    """åˆ†æç›®å½•ä¸‹æ‰€æœ‰JSONæ–‡ä»¶"""
    json_files = glob.glob(os.path.join(directory, '*.json'))

    # æ’é™¤READMEå’Œsummaryæ–‡ä»¶
    json_files = [f for f in json_files if not any(x in os.path.basename(f) for x in ['README', 'summary'])]

    analysis_results = []

    print(f"ğŸ“Š Analyzing {len(json_files)} JSON files...")

    for filepath in json_files:
        filename = os.path.basename(filepath)
        print(f"ğŸ” Analyzing: {filename}")

        result = analyze_json_file(filepath)
        result['filename'] = filename
        result['filepath'] = filepath
        analysis_results.append(result)

        status = "âœ… Valid" if result['valid'] else "âŒ Invalid"
        reason = result.get('reason', '')
        size = result.get('file_size', 0)

        print(f"   {status} - Size: {size:,} bytes - {reason}")

    return analysis_results

def remove_invalid_files(results, dry_run=True):
    """ç§»é™¤æ— æ•ˆæ–‡ä»¶"""
    invalid_files = [r for r in results if not r['valid']]

    print(f"\nğŸ—‘ï¸  Found {len(invalid_files)} invalid files")

    if not invalid_files:
        print("âœ… All files are valid!")
        return []

    if dry_run:
        print("\nğŸ” DRY RUN - Files that would be removed:")
        for result in invalid_files:
            print(f"   - {result['filename']} ({result['file_size']} bytes) - {result['reason']}")
        return invalid_files

    print("\nğŸ—‘ï¸  Removing invalid files...")
    removed_files = []

    for result in invalid_files:
        try:
            os.remove(result['filepath'])
            removed_files.append(result['filename'])
            print(f"   âœ… Removed: {result['filename']}")
        except Exception as e:
            print(f"   âŒ Failed to remove {result['filename']}: {e}")

    return removed_files

def generate_cleanup_report(results, removed_files=None):
    """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
    valid_files = [r for r in results if r['valid']]
    invalid_files = [r for r in results if not r['valid']]

    # ç»Ÿè®¡ä¿¡æ¯
    total_size = sum(r['file_size'] for r in results)
    valid_size = sum(r['file_size'] for r in valid_files)
    invalid_size = sum(r['file_size'] for r in invalid_files)

    # æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
    model_stats = {}
    for result in valid_files:
        metadata = result.get('metadata', {})
        model = metadata.get('model', 'Unknown')
        if model not in model_stats:
            model_stats[model] = {'files': 0, 'size': 0, 'data_points': 0}
        model_stats[model]['files'] += 1
        model_stats[model]['size'] += result['file_size']
        model_stats[model]['data_points'] += result.get('valid_data_points', 0)

    report = {
        'cleanup_timestamp': os.path.getmtime('.'),
        'summary': {
            'total_files': len(results),
            'valid_files': len(valid_files),
            'invalid_files': len(invalid_files),
            'removed_files': len(removed_files) if removed_files else 0,
            'total_size_bytes': total_size,
            'valid_size_bytes': valid_size,
            'invalid_size_bytes': invalid_size,
            'space_saved_bytes': invalid_size
        },
        'model_statistics': model_stats,
        'valid_files_detail': [
            {
                'filename': r['filename'],
                'model': r.get('metadata', {}).get('model', 'Unknown'),
                'sequence': r.get('metadata', {}).get('sequence', 'Unknown'),
                'file_size_bytes': r['file_size'],
                'data_points': r.get('valid_data_points', 0)
            } for r in valid_files
        ],
        'invalid_files_detail': [
            {
                'filename': r['filename'],
                'file_size_bytes': r['file_size'],
                'reason': r['reason']
            } for r in invalid_files
        ]
    }

    # ä¿å­˜æŠ¥å‘Š
    report_path = 'json_data/raw_json_files/cleanup_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    # ç”Ÿæˆå¯è¯»æ‘˜è¦
    readable_report = f"""# JSONæ–‡ä»¶æ¸…ç†æŠ¥å‘Š

## ğŸ“Š æ€»ä½“ç»Ÿè®¡
- **åŸå§‹æ–‡ä»¶æ€»æ•°**: {len(results)}
- **æœ‰æ•ˆæ–‡ä»¶æ•°**: {len(valid_files)}
- **æ— æ•ˆæ–‡ä»¶æ•°**: {len(invalid_files)}
- **å·²åˆ é™¤æ–‡ä»¶æ•°**: {len(removed_files) if removed_files else 0}
- **åŸå§‹æ€»å¤§å°**: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)
- **æœ‰æ•ˆæ•°æ®å¤§å°**: {valid_size:,} bytes ({valid_size/1024/1024:.2f} MB)
- **èŠ‚çœç©ºé—´**: {invalid_size:,} bytes ({invalid_size/1024/1024:.2f} MB)

## ğŸ–¥ï¸ æŒ‰æ¨¡å‹ç»Ÿè®¡
"""

    for model, stats in model_stats.items():
        readable_report += f"""
### {model}
- **æ–‡ä»¶æ•°**: {stats['files']}
- **æ•°æ®å¤§å°**: {stats['size']:,} bytes ({stats['size']/1024/1024:.2f} MB)
- **æ•°æ®ç‚¹æ•°**: {stats['data_points']}
"""

    readable_report += f"""
## âœ… æœ‰æ•ˆæ–‡ä»¶åˆ—è¡¨ ({len(valid_files)}ä¸ª)
"""

    for r in valid_files:
        metadata = r.get('metadata', {})
        model = metadata.get('model', 'Unknown')
        sequence = metadata.get('sequence', 'Unknown')
        readable_report += f"- **{r['filename']}** - {model} + {sequence} ({r['file_size']:,} bytes, {r.get('valid_data_points', 0)} æ•°æ®ç‚¹)\n"

    if invalid_files:
        readable_report += f"""
## âŒ æ— æ•ˆæ–‡ä»¶åˆ—è¡¨ ({len(invalid_files)}ä¸ª)
"""
        for r in invalid_files:
            readable_report += f"- **{r['filename']}** - {r['file_size']:,} bytes - {r['reason']}\n"

    # ä¿å­˜å¯è¯»æŠ¥å‘Š
    readable_report_path = 'json_data/raw_json_files/CLEANUP_REPORT.md'
    with open(readable_report_path, 'w', encoding='utf-8') as f:
        f.write(readable_report)

    print(f"\nğŸ“‹ Reports generated:")
    print(f"   - JSON report: {report_path}")
    print(f"   - Readable report: {readable_report_path}")

    return report

def main():
    directory = 'json_data/raw_json_files'

    if not os.path.exists(directory):
        print(f"âŒ Directory {directory} does not exist")
        return

    print("ğŸš€ Starting JSON file cleanup analysis...")

    # åˆ†ææ‰€æœ‰æ–‡ä»¶
    results = analyze_all_files(directory)

    # ç”Ÿæˆåˆå§‹æŠ¥å‘Š
    print("\nğŸ“Š Generating initial analysis report...")
    generate_cleanup_report(results)

    # å…ˆè¿›è¡Œå¹²è¿è¡Œ
    print("\nğŸ” Performing dry run...")
    invalid_files = remove_invalid_files(results, dry_run=True)

    if not invalid_files:
        print("\nâœ… No invalid files found. All files are valid!")
        return

    # è¯¢é—®æ˜¯å¦ç»§ç»­åˆ é™¤
    print(f"\nâš ï¸  Found {len(invalid_files)} invalid files.")
    print("These files will be permanently deleted.")

    # è‡ªåŠ¨åˆ é™¤æ— æ•ˆæ–‡ä»¶ï¼ˆæ ¹æ®éœ€æ±‚ï¼‰
    print("\nğŸ—‘ï¸  Removing invalid files...")
    removed_files = remove_invalid_files(results, dry_run=False)

    if removed_files:
        print(f"\nâœ… Successfully removed {len(removed_files)} invalid files")

        # é‡æ–°åˆ†ææ¸…ç†åçš„æ–‡ä»¶
        print("\nğŸ“Š Re-analyzing files after cleanup...")
        remaining_files = glob.glob(os.path.join(directory, '*.json'))
        remaining_files = [f for f in remaining_files if not any(x in os.path.basename(f) for x in ['README', 'summary', 'cleanup', 'CLEANUP'])]

        final_results = []
        for filepath in remaining_files:
            result = analyze_json_file(filepath)
            result['filename'] = os.path.basename(filepath)
            result['filepath'] = filepath
            final_results.append(result)

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        generate_cleanup_report(final_results, removed_files)

        print(f"\nğŸ‰ Cleanup completed!")
        print(f"ğŸ“ Remaining valid files: {len(final_results)}")
        print(f"ğŸ’¾ Total valid data size: {sum(r['file_size'] for r in final_results):,} bytes")

if __name__ == "__main__":
    main()