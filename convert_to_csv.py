#!/usr/bin/env python3
import json
import os
import glob
import csv
import re
from datetime import datetime

def normalize_sequence_format(sequence_str):
    """å°†åºåˆ—æ ¼å¼æ ‡å‡†åŒ–ä¸º 1k-1k, 1k-8k ç­‰æ ¼å¼"""
    # ç§»é™¤ç©ºæ ¼å¹¶è½¬æ¢ä¸ºå°å†™
    normalized = sequence_str.replace(' ', '').lower()
    # å°† "/" æ›¿æ¢ä¸º "-"
    normalized = normalized.replace('/', '-')
    return normalized

def extract_all_fields(json_files):
    """æå–æ‰€æœ‰å¯èƒ½çš„å­—æ®µå"""
    all_fields = set()
    all_nested_fields = {}

    for filepath in json_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            data_points = data.get('data', [])
            for point in data_points:
                if isinstance(point, dict):
                    for key, value in point.items():
                        if isinstance(value, dict):
                            # åµŒå¥—å¯¹è±¡ï¼Œæå–å…¶å­—æ®µ
                            for nested_key in value.keys():
                                all_nested_fields[key] = all_nested_fields.get(key, set())
                                all_nested_fields[key].add(nested_key)
                        else:
                            all_fields.add(key)
        except Exception as e:
            print(f"Error extracting fields from {filepath}: {e}")

    # å°†åµŒå¥—å­—æ®µè½¬æ¢ä¸ºæ‰å¹³å­—æ®µå
    flat_fields = set(all_fields)
    for parent_key, nested_keys in all_nested_fields.items():
        for nested_key in nested_keys:
            flat_fields.add(f"{parent_key}_{nested_key}")

    return sorted(list(flat_fields))

def flatten_data_point(data_point):
    """å°†åµŒå¥—çš„æ•°æ®ç‚¹æ‰å¹³åŒ–"""
    flattened = {}

    for key, value in data_point.items():
        if isinstance(value, dict):
            # å¤„ç†åµŒå¥—å¯¹è±¡
            for nested_key, nested_value in value.items():
                flattened[f"{key}_{nested_key}"] = nested_value
        else:
            flattened[key] = value

    return flattened

def process_json_files(directory):
    """å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰JSONæ–‡ä»¶"""
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶ï¼ˆæŽ’é™¤æŠ¥å‘Šæ–‡ä»¶ï¼‰
    json_files = glob.glob(os.path.join(directory, '*.json'))
    json_files = [f for f in json_files if not any(x in os.path.basename(f).lower()
                                               for x in ['readme', 'summary', 'cleanup', 'report'])]

    print(f"ðŸ“Š Found {len(json_files)} JSON files to process")

    if not json_files:
        print("âŒ No JSON files found!")
        return None

    # æå–æ‰€æœ‰å¯èƒ½çš„å­—æ®µ
    print("ðŸ” Analyzing data structure...")
    all_fields = extract_all_fields(json_files)
    print(f"ðŸ“‹ Found {len(all_fields)} unique data fields")

    # å®šä¹‰CSVåˆ—çš„é¡ºåº
    csv_columns = ['model_name', 'sequence_length'] + all_fields

    # å‡†å¤‡CSVæ•°æ®
    csv_data = []
    total_records = 0

    print("ðŸ”„ Processing JSON files...")
    for i, filepath in enumerate(json_files):
        filename = os.path.basename(filepath)
        print(f"  ðŸ“„ Processing {i+1}/{len(json_files)}: {filename}")

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                json_data = json.load(f)

            # æå–å…ƒæ•°æ®
            metadata = json_data.get('metadata', {})
            model_name = metadata.get('model', 'Unknown')
            sequence_length = normalize_sequence_format(metadata.get('sequence', 'Unknown'))

            # å¤„ç†æ•°æ®ç‚¹
            data_points = json_data.get('data', [])
            file_records = 0

            for data_point in data_points:
                if isinstance(data_point, dict):
                    # æ‰å¹³åŒ–æ•°æ®ç‚¹
                    flattened_point = flatten_data_point(data_point)

                    # åˆ›å»ºCSVè¡Œ
                    csv_row = {}

                    # æ·»åŠ æ¨¡åž‹å’Œåºåˆ—åˆ—
                    csv_row['model_name'] = model_name
                    csv_row['sequence_length'] = sequence_length

                    # æ·»åŠ æ‰€æœ‰æ•°æ®å­—æ®µ
                    for field in all_fields:
                        csv_row[field] = flattened_point.get(field, '')

                    csv_data.append(csv_row)
                    file_records += 1

            total_records += file_records
            print(f"    âœ… Extracted {file_records} data points")

        except Exception as e:
            print(f"    âŒ Error processing {filename}: {e}")

    print(f"ðŸ“Š Total records extracted: {total_records}")

    return csv_columns, csv_data

def generate_csv_summary(csv_data, output_file):
    """ç”ŸæˆCSVæ•°æ®çš„ç»Ÿè®¡æ‘˜è¦"""
    if not csv_data:
        return

    # ç»Ÿè®¡ä¿¡æ¯
    total_records = len(csv_data)
    models = set()
    sequences = set()
    hardware = set()
    precisions = set()

    for row in csv_data:
        models.add(row.get('model_name', 'Unknown'))
        sequences.add(row.get('sequence_length', 'Unknown'))
        if row.get('hwKey'):
            hardware.add(row['hwKey'])
        if row.get('precision'):
            precisions.add(row['precision'])

    summary = {
        'generation_timestamp': datetime.now().isoformat(),
        'output_file': output_file,
        'total_records': total_records,
        'unique_models': list(models),
        'unique_sequences': list(sequences),
        'unique_hardware': list(hardware),
        'unique_precisions': list(precisions),
        'csv_columns': len(csv_data[0]) if csv_data else 0
    }

    return summary

def save_csv_file(csv_columns, csv_data, output_file):
    """ä¿å­˜CSVæ–‡ä»¶"""
    try:
        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=csv_columns)

            # å†™å…¥è¡¨å¤´
            writer.writeheader()

            # å†™å…¥æ•°æ®è¡Œ
            writer.writerows(csv_data)

        print(f"âœ… CSV file saved: {output_file}")
        return True

    except Exception as e:
        print(f"âŒ Error saving CSV file: {e}")
        return False

def create_readable_summary(summary, csv_columns):
    """åˆ›å»ºå¯è¯»çš„æ‘˜è¦æ–‡ä»¶"""
    summary_text = f"""# InferenceMAX æ•°æ®é›† CSV è½¬æ¢æŠ¥å‘Š

## ðŸ“Š è½¬æ¢ç»Ÿè®¡
- **ç”Ÿæˆæ—¶é—´**: {summary['generation_timestamp']}
- **è¾“å‡ºæ–‡ä»¶**: {summary['output_file']}
- **æ€»è®°å½•æ•°**: {summary['total_records']:,}
- **CSVåˆ—æ•°**: {summary['csv_columns']}

## ðŸ–¥ï¸ æ•°æ®æ¦‚è§ˆ
- **æ¨¡åž‹æ•°é‡**: {len(summary['unique_models'])}
- **åºåˆ—é…ç½®**: {len(summary['unique_sequences'])}
- **ç¡¬ä»¶å¹³å°**: {len(summary['unique_hardware'])}
- **ç²¾åº¦ç±»åž‹**: {len(summary['unique_precisions'])}

## ðŸ“‹ æ•°æ®è¯¦æƒ…

### æ¨¡åž‹åˆ—è¡¨
"""
    for model in sorted(summary['unique_models']):
        summary_text += f"- {model}\n"

    summary_text += f"""
### åºåˆ—é…ç½®
"""
    for seq in sorted(summary['unique_sequences']):
        summary_text += f"- {seq}\n"

    summary_text += f"""
### ç¡¬ä»¶å¹³å°
"""
    for hw in sorted(summary['unique_hardware']):
        summary_text += f"- {hw}\n"

    summary_text += f"""
### ç²¾åº¦ç±»åž‹
"""
    for precision in sorted(summary['unique_precisions']):
        summary_text += f"- {precision}\n"

    summary_text += f"""
## ðŸ“„ CSV åˆ—è¯´æ˜Ž
å‰ä¸¤åˆ—ä¸ºæ ‡è¯†åˆ—ï¼š
- **model_name**: æ¨¡åž‹åç§°
- **sequence_length**: åºåˆ—é•¿åº¦é…ç½®

æ•°æ®åˆ—åŒ…å«æ€§èƒ½æŒ‡æ ‡ï¼š
- **conc**: å¹¶å‘æ•°
- **hwKey**: ç¡¬ä»¶å¹³å°æ ‡è¯†
- **precision**: ç²¾åº¦ç±»åž‹ (FP8/FP4)
- **tp**: å¼ é‡å¹¶è¡Œæ•°
- **costh_y**: P50å»¶è¿Ÿ (æ¯«ç§’)
- **costn_y**: P90å»¶è¿Ÿ (æ¯«ç§’)
- **costr_y**: P99å»¶è¿Ÿ (æ¯«ç§’)
- **tpPerGpu_y**: æ¯GPUåžåé‡ (tokens/second)
- **tpPerMw_y**: æ¯å…†ç“¦åžåé‡
- **x**: Xè½´åæ ‡ (å»¶è¿Ÿ)
- **y**: Yè½´åæ ‡ (åžåé‡)
- **costh_roof**, **costn_roof**, **costr_roof**: æ˜¯å¦è¾¾åˆ°æ€§èƒ½ä¸Šé™
- **tpPerGpu_roof**, **tpPerMw_roof**: æ˜¯å¦è¾¾åˆ°æ€§èƒ½ä¸Šé™

## ðŸ” ä½¿ç”¨å»ºè®®
1. **æ€§èƒ½æ¯”è¾ƒ**: æŒ‰model_nameåˆ†ç»„æ¯”è¾ƒä¸åŒç¡¬ä»¶çš„æ€§èƒ½
2. **åºåˆ—å½±å“**: åˆ†æžsequence_lengthå¯¹æ€§èƒ½çš„å½±å“
3. **ç¡¬ä»¶é€‰æ‹©**: æ ¹æ®hwKeyé€‰æ‹©æœ€é€‚åˆçš„ç¡¬ä»¶å¹³å°
4. **ç²¾åº¦æƒè¡¡**: æ¯”è¾ƒprecisionä¸ºfp8å’Œfp4çš„æ€§èƒ½å·®å¼‚

---

*æ­¤CSVæ–‡ä»¶ç”± InferenceMAX JSONæ•°æ®è½¬æ¢ç”Ÿæˆ*
"""

    return summary_text

def main():
    input_directory = 'json_data/raw_json_files'
    output_file = 'json_data/inference_max_dataset.csv'
    summary_file = 'json_data/CSV_CONVERSION_REPORT.md'

    if not os.path.exists(input_directory):
        print(f"âŒ Input directory {input_directory} does not exist")
        return

    print("ðŸš€ Starting JSON to CSV conversion...")

    # å¤„ç†JSONæ–‡ä»¶
    result = process_json_files(input_directory)
    if result is None:
        print("âŒ No data to convert")
        return

    csv_columns, csv_data = result

    if not csv_data:
        print("âŒ No data records found")
        return

    # ä¿å­˜CSVæ–‡ä»¶
    print(f"\nðŸ’¾ Saving CSV file...")
    if save_csv_file(csv_columns, csv_data, output_file):
        # ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
        summary = generate_csv_summary(csv_data, output_file)

        # åˆ›å»ºå¯è¯»æ‘˜è¦
        readable_summary = create_readable_summary(summary, csv_columns)

        # ä¿å­˜æ‘˜è¦æ–‡ä»¶
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(readable_summary)

        print(f"ðŸ“‹ Summary report saved: {summary_file}")

        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(output_file)
        print(f"\nðŸŽ‰ Conversion completed successfully!")
        print(f"ðŸ“„ Output file: {output_file}")
        print(f"ðŸ“Š File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")
        print(f"ðŸ“ˆ Total records: {len(csv_data):,}")
        print(f"ðŸ“‹ Columns: {len(csv_columns)}")

        # æ˜¾ç¤ºå‰å‡ è¡Œä½œä¸ºé¢„è§ˆ
        print(f"\nðŸ“‹ Column preview:")
        print("Columns:", ", ".join(csv_columns[:10]) + "..." if len(csv_columns) > 10 else ", ".join(csv_columns))

        print(f"\nðŸ“Š Data preview (first 3 records):")
        for i, row in enumerate(csv_data[:3]):
            print(f"Record {i+1}:")
            print(f"  Model: {row.get('model_name', 'N/A')}")
            print(f"  Sequence: {row.get('sequence_length', 'N/A')}")
            print(f"  Hardware: {row.get('hwKey', 'N/A')}")
            print(f"  Precision: {row.get('precision', 'N/A')}")
            print(f"  Concurrency: {row.get('conc', 'N/A')}")
            print(f"  Throughput: {row.get('tpPerGpu_y', 'N/A')}")
            print()

if __name__ == "__main__":
    main()